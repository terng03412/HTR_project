{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# HTR Dependency\n",
    "import cv2\n",
    "import string\n",
    "import h5py\n",
    "\n",
    "keras = tf.keras\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1.0-dev20191224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "DeepSpell based text cleaning process.\n",
    "    Tal Weiss.\n",
    "    Deep Spelling.\n",
    "    Medium: https://machinelearnings.co/deep-spelling-9ffef96a24f6#.2c9pu8nlm\n",
    "    Github: https://github.com/MajorTal/DeepSpell\n",
    "\"\"\"\n",
    "\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
    "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
    "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
    "\n",
    "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
    "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
    "\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
    "\n",
    "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
    "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
    "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
    "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
    "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
    "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
    "\n",
    "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
    "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
    "    text = text.translate(str.maketrans({c: f\" {c} \" for c in string.punctuation}))\n",
    "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
    "\n",
    "    return text\n",
    "\n",
    "\"\"\"\n",
    "Sauvola binarization based in,\n",
    "    J. Sauvola, T. Seppanen, S. Haapakoski, M. Pietikainen,\n",
    "    Adaptive Document Binarization, in IEEE Computer Society Washington, 1997.\n",
    "\"\"\"\n",
    "\n",
    "def sauvola(img, window, thresh, k):\n",
    "    \"\"\"Sauvola binarization\"\"\"\n",
    "\n",
    "    rows, cols = img.shape\n",
    "    pad = int(np.floor(window[0] / 2))\n",
    "    sum2, sqsum = cv2.integral2(\n",
    "        cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_CONSTANT))\n",
    "\n",
    "    isum = sum2[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
    "        sum2[0:rows, 0:cols] - \\\n",
    "        sum2[window[0]:rows + window[0], 0:cols] - \\\n",
    "        sum2[0:rows, window[1]:cols + window[1]]\n",
    "\n",
    "    isqsum = sqsum[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
    "        sqsum[0:rows, 0:cols] - \\\n",
    "        sqsum[window[0]:rows + window[0], 0:cols] - \\\n",
    "        sqsum[0:rows, window[1]:cols + window[1]]\n",
    "\n",
    "    ksize = window[0] * window[1]\n",
    "    mean = isum / ksize\n",
    "    std = (((isqsum / ksize) - (mean**2) / ksize) / ksize) ** 0.5\n",
    "    threshold = (mean * (1 + k * (std / thresh - 1))) * (mean >= 100)\n",
    "\n",
    "    return np.asarray(255 * (img >= threshold), 'uint8')\n",
    "\n",
    "def remove_cursive_style(img):\n",
    "    \"\"\"Remove cursive writing style from image with deslanting algorithm\"\"\"\n",
    "\n",
    "    def calc_y_alpha(vec):\n",
    "        indices = np.where(vec > 0)[0]\n",
    "        h_alpha = len(indices)\n",
    "\n",
    "        if h_alpha > 0:\n",
    "            delta_y_alpha = indices[h_alpha - 1] - indices[0] + 1\n",
    "\n",
    "            if h_alpha == delta_y_alpha:\n",
    "                return h_alpha * h_alpha\n",
    "        return 0\n",
    "\n",
    "    alpha_vals = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    rows, cols = img.shape\n",
    "    results = []\n",
    "\n",
    "    ret, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    binary = otsu if ret < 127 else sauvola(img, (int(img.shape[0] / 2), int(img.shape[0] / 2)), 127, 1e-2)\n",
    "\n",
    "    for alpha in alpha_vals:\n",
    "        shift_x = max(-alpha * rows, 0.)\n",
    "        size = (cols + int(np.ceil(abs(alpha * rows))), rows)\n",
    "        transform = np.asarray([[1, alpha, shift_x], [0, 1, 0]], dtype=np.float)\n",
    "\n",
    "        shear_img = cv2.warpAffine(binary, transform, size, cv2.INTER_NEAREST)\n",
    "        sum_alpha = 0\n",
    "        sum_alpha += np.apply_along_axis(calc_y_alpha, 0, shear_img)\n",
    "        results.append([np.sum(sum_alpha), size, transform])\n",
    "\n",
    "    result = sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "    warp = cv2.warpAffine(img, result[2], result[1], borderValue=255)\n",
    "\n",
    "    return cv2.resize(warp, dsize=(cols, rows))\n",
    "\n",
    "def preproc(img, input_size):\n",
    "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
    "    img_src = img\n",
    "    if isinstance(img, str):\n",
    "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if isinstance(img, tuple):\n",
    "        image, boundbox = img\n",
    "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        for i in range(len(boundbox)):\n",
    "            if isinstance(boundbox[i], float):\n",
    "                total = len(img) if i < 2 else len(img[0])\n",
    "                boundbox[i] = int(total * boundbox[i])\n",
    "\n",
    "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
    "\n",
    "    wt, ht, _ = input_size\n",
    "    try:\n",
    "        h, w = np.asarray(img).shape\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {img_src}\")\n",
    "        return\n",
    "\n",
    "    f = max((w / wt), (h / ht))\n",
    "\n",
    "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
    "    img = cv2.resize(img, new_size)\n",
    "\n",
    "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    if np.sum(img) * 0.8 > np.sum(binary):\n",
    "        img = illumination_compensation(img)\n",
    "\n",
    "    img = remove_cursive_style(img)\n",
    "\n",
    "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
    "    target[0:new_size[1], 0:new_size[0]] = img\n",
    "    img = cv2.transpose(target)\n",
    "\n",
    "    return img\n",
    "\n",
    "def normalization(imgs):\n",
    "    \"\"\"Normalize list of images\"\"\"\n",
    "\n",
    "    imgs = np.asarray(imgs).astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        m, s = cv2.meanStdDev(imgs[i])\n",
    "        imgs[i] = imgs[i] - m[0][0]\n",
    "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
    "\n",
    "    return np.expand_dims(imgs, axis=-1)\n",
    "\n",
    "\n",
    "def augmentation(imgs,\n",
    "                 rotation_range=0,\n",
    "                 scale_range=0,\n",
    "                 height_shift_range=0,\n",
    "                 width_shift_range=0,\n",
    "                 dilate_range=1,\n",
    "                 erode_range=1):\n",
    "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
    "\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
    "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
    "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
    "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
    "    scale = np.random.uniform(1 - scale_range, 1)\n",
    "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
    "\n",
    "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
    "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
    "\n",
    "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
    "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
    "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
    "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
    "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
    "\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, batch_size, charset, max_text_length, predict=False):\n",
    "        self.tokenizer = Tokenizer(charset, max_text_length)\n",
    "        self.batch_size = batch_size\n",
    "        self.partitions = ['test'] if predict else ['train', 'valid', 'test']\n",
    "\n",
    "        self.size = dict()\n",
    "        self.steps = dict()\n",
    "        self.index = dict()\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            for pt in self.partitions:\n",
    "                self.dataset[pt] = dict()\n",
    "                self.dataset[pt]['dt'] = f[pt]['dt'][:]\n",
    "                self.dataset[pt]['gt'] = f[pt]['gt'][:]\n",
    "\n",
    "        for pt in self.partitions:\n",
    "            # decode sentences from byte\n",
    "            self.dataset[pt]['gt'] = [x.decode() for x in self.dataset[pt]['gt']]\n",
    "\n",
    "            # set size and setps\n",
    "            self.size[pt] = len(self.dataset[pt]['gt'])\n",
    "            self.steps[pt] = int(np.ceil(self.size[pt] / self.batch_size))\n",
    "            self.index[pt] = 0\n",
    "\n",
    "    def next_train_batch(self):\n",
    "        \"\"\"Get the next batch from train partition (yield)\"\"\"\n",
    "\n",
    "        while True:\n",
    "            if self.index['train'] >= self.size['train']:\n",
    "                self.index['train'] = 0\n",
    "\n",
    "            index = self.index['train']\n",
    "            until = self.index['train'] + self.batch_size\n",
    "            self.index['train'] = until\n",
    "\n",
    "            x_train = self.dataset['train']['dt'][index:until]\n",
    "            y_train = self.dataset['train']['gt'][index:until]\n",
    "\n",
    "            x_train = augmentation(x_train,\n",
    "                                      rotation_range=1.5,\n",
    "                                      scale_range=0.05,\n",
    "                                      height_shift_range=0.025,\n",
    "                                      width_shift_range=0.05,\n",
    "                                      erode_range=5,\n",
    "                                      dilate_range=3)\n",
    "\n",
    "            x_train = normalization(x_train)\n",
    "\n",
    "            y_train = [self.tokenizer.encode(y) for y in y_train]\n",
    "            y_train = pad_sequences(y_train, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
    "\n",
    "            yield (x_train, y_train, [])\n",
    "\n",
    "    def next_valid_batch(self):\n",
    "        \"\"\"Get the next batch from validation partition (yield)\"\"\"\n",
    "\n",
    "        while True:\n",
    "            if self.index['valid'] >= self.size['valid']:\n",
    "                self.index['valid'] = 0\n",
    "\n",
    "            index = self.index['valid']\n",
    "            until = self.index['valid'] + self.batch_size\n",
    "            self.index['valid'] = until\n",
    "\n",
    "            x_valid = self.dataset['valid']['dt'][index:until]\n",
    "            y_valid = self.dataset['valid']['gt'][index:until]\n",
    "\n",
    "            x_valid = normalization(x_valid)\n",
    "\n",
    "            y_valid = [self.tokenizer.encode(y) for y in y_valid]\n",
    "            y_valid = pad_sequences(y_valid, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
    "\n",
    "            yield (x_valid, y_valid, [])\n",
    "\n",
    "    def next_test_batch(self):\n",
    "        \"\"\"Return model predict parameters\"\"\"\n",
    "\n",
    "        while True:\n",
    "            if self.index['test'] >= self.size['test']:\n",
    "                self.index['test'] = 0\n",
    "                break\n",
    "\n",
    "            index = self.index['test']\n",
    "            until = self.index['test'] + self.batch_size\n",
    "            self.index['test'] = until\n",
    "\n",
    "            x_test = self.dataset['test']['dt'][index:until]\n",
    "            x_test = normalization(x_test)\n",
    "\n",
    "            yield x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\"\n",
    "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
    "\n",
    "        self.PAD = self.chars.find(self.PAD_TK)\n",
    "        self.UNK = self.chars.find(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "        encoded = []\n",
    "\n",
    "        for item in text:\n",
    "            index = self.chars.find(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "\n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/home/kuadmin01/terng/'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = \" \"\n",
    "SPECIAL_CHARS = \"?!,.\"\n",
    "ALPHANUMERIC = string.printable[:62]\n",
    "CHARS = ALPHANUMERIC + SPECIAL_CHARS + SPACE\n",
    "CHARS\n",
    "\n",
    "INPUT_SOURCE_NAME = \"iam_word\"\n",
    "BATCH_SIZE = 16\n",
    "MAX_TEXT_LENGTH = 32\n",
    "CHARSET_BASE = CHARS\n",
    "\n",
    "dtgen = DataGenerator(\n",
    "          source=f\"/home/kuadmin01/terng/Dataset/dataset_for_experiment.hdf5\",\n",
    "          batch_size=BATCH_SIZE,\n",
    "          charset=CHARSET_BASE,\n",
    "          max_text_length=MAX_TEXT_LENGTH,\n",
    "          predict=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    \"\"\"Function for computing the CTC loss\"\"\"\n",
    "    \n",
    "    if len(y_true.shape) > 2:\n",
    "        y_true = tf.squeeze(y_true)\n",
    "\n",
    "    input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
    "    input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
    "    label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.HTR_Models import FlorHTR, SmallFlorHTR, PuigCerver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (1024, 128, 1)\n",
    "OUTPUT_SHAPE = dtgen.tokenizer.vocab_size + 1\n",
    "\n",
    "inputs, outputs = FlorHTR(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=5e-4), loss=ctc_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Callback(source, model_name):\n",
    "    \n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    callbacks = [\n",
    "    ModelCheckpoint(\n",
    "      filepath=f\"target/\"+ str(model_name) + \"/\" + str(source)  + \"_checkpoint_weights.hdf5\",\n",
    "      monitor=\"val_loss\",\n",
    "      save_best_only=True,\n",
    "      save_weights_only=True,\n",
    "      verbose=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "      monitor=\"val_loss\",\n",
    "      min_delta=1e-8,\n",
    "      patience=20,\n",
    "      restore_best_weights=True,\n",
    "      verbose=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "      monitor=\"val_loss\",\n",
    "      min_delta=1e-8,\n",
    "      factor=0.2,\n",
    "      patience=15,\n",
    "      verbose=True\n",
    "    ),\n",
    "    CSVLogger(\n",
    "      filename=f\"log/\"+ str(model_name) + \"/\" + str(source)  + \"_epochs.log\",\n",
    "      separator=\";\",\n",
    "      append=True\n",
    "    ),\n",
    "      tensorboard_callback\n",
    "    ]\n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = Callback(INPUT_SOURCE_NAME, 'Flor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(f\"/home/kuadmin01/terng/SeniorProjectMaterial/target/Flor/iam_word_checkpoint_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SeniorProjectMaterial/\n",
    "EPOCHS = 10\n",
    "history = model.fit(x=dtgen.next_train_batch(),\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=dtgen.steps['train'],\n",
    "            validation_data=dtgen.next_valid_batch(),\n",
    "            validation_steps=dtgen.steps['valid'],\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True,\n",
    "            verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./logs/ \n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6010 (pid 109167), started 0:04:24 ago. (Use '!kill 109167' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-196c82c0148daf67\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-196c82c0148daf67\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/Terng_HTR3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 47757), started 5 days, 20:15:44 ago. (Use '!kill 47757' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e154ae82d3793041\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e154ae82d3793041\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%tensorboard --logdir logs/Flor/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd SeniorProjectMaterial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save(f\"saved_model/Flor/{INPUT_SOURCE_NAME}_htr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(f\"saved_model/Flor/{INPUT_SOURCE_NAME}_htr\")\n",
    "\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "#                                        tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "converter.experimental_new_converter = True\n",
    "tflite_model = converter.convert()\n",
    "open(f\"{INPUT_SOURCE_NAME}_Flor_htr.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SeniorProjectMaterial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_IMAGE_SRC = \"hello.png\"\n",
    "tokenizer = Tokenizer(chars=CHARS, max_text_length=MAX_TEXT_LENGTH)\n",
    "img = preproc(PREDICT_IMAGE_SRC, input_size=INPUT_SHAPE)\n",
    "x_test = normalization([ img ])\n",
    "\n",
    "STEPS = 1\n",
    "\n",
    "out = model.predict(\n",
    "        x=x_test,\n",
    "        batch_size=None,\n",
    "        verbose=False,\n",
    "        steps=STEPS,\n",
    "        callbacks=None, \n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False\n",
    "      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "batch_size = int(np.ceil(len(out) / STEPS))\n",
    "input_length = len(max(out, key=len))\n",
    "predicts, probabilities = [], []\n",
    "\n",
    "while steps_done < STEPS:\n",
    "    index = steps_done * batch_size\n",
    "    until = index + batch_size\n",
    "\n",
    "    x_test = np.asarray(out[index:until])\n",
    "    x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
    "\n",
    "    decode, log = keras.backend.ctc_decode(\n",
    "                  x_test,\n",
    "                  x_test_len,\n",
    "                  greedy=True,\n",
    "                  beam_width=10,\n",
    "                  top_paths=3\n",
    "                )\n",
    "\n",
    "    probabilities.extend([np.exp(x) for x in log])\n",
    "    decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
    "    predicts.extend(np.swapaxes(decode, 0, 1))\n",
    "    # update step\n",
    "    steps_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "steps_done = 0\n",
    "batch_size = int(np.ceil(len(out) / STEPS))\n",
    "input_length = len(max(out, key=len))\n",
    "predicts, probabilities = [], []\n",
    "\n",
    "while steps_done < STEPS:\n",
    "    index = steps_done * batch_size\n",
    "    until = index + batch_size\n",
    "\n",
    "    x_test = np.asarray(out[index:until])\n",
    "    x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
    "\n",
    "    decode, log = tf.nn.ctc_beam_search_decoder(\n",
    "                  x_test,\n",
    "                  128\n",
    "                )\n",
    "\n",
    "    probabilities.extend([np.exp(x) for x in log])\n",
    "    decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
    "    predicts.extend(np.swapaxes(decode, 0, 1))\n",
    "    # update step\n",
    "    steps_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = [[tokenizer.decode(x) for x in y] for y in predicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n####################################\")\n",
    "for i, (pred, prob) in enumerate(zip(predicts, probabilities)):\n",
    "  print(\"\\nProb.  - Predict\")\n",
    "  for (pd, pb) in zip(pred, prob):\n",
    "    print(f\"{pb:.4f} - {pd}\")\n",
    "print(\"\\n####################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 141931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
