{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2  # Not actually necessary if you just want to create an image.\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image  \n",
    "import h5py\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def generate(text, filepath, fontpath):\n",
    "    height = 100\n",
    "    width = 1\n",
    "    blank_image = np.zeros((height,width,3), np.uint8)\n",
    "    blank_image[:,:] = (255,255,255)  \n",
    "    image = blank_image  \n",
    "    # Convert the image to RGB (OpenCV uses BGR)  \n",
    "    cv2_im_rgb = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)  \n",
    "\n",
    "    # Pass the image to PIL  \n",
    "    pil_im = Image.fromarray(cv2_im_rgb)  \n",
    "\n",
    "    draw = ImageDraw.Draw(pil_im)  \n",
    "    # use a truetype font  \n",
    "    font = ImageFont.truetype(fontpath, 100)  \n",
    "\n",
    "\n",
    "    # Draw the text  \n",
    "    draw.text((4,0), text, font=font, fill=(0,0,0))\n",
    "\n",
    "    # Get back the image to OpenCV  \n",
    "    cv2_im_processed = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)  \n",
    "#     cv2.imshow(cv2_im_processed) \n",
    "    cv2.imwrite(filepath, cv2_im_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_list = ['alphabetizedCassetteTapes.ttf','ASensibleArmadillo.ttf','ArdiniaDemo.ttf','Buttercake.ttf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_font = \"/home/kuadmin01/terng/Fonts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'aa'\n",
    "for i in range(len(font_list)):\n",
    "    text += 'a'\n",
    "    generate(text, './test' + str(i)+ '.png',path_to_font+font_list[i] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncateLabel(text, maxTextLen):\n",
    "        # ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
    "        # labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "        # If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i-1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > maxTextLen:\n",
    "                return text[:i]\n",
    "        return text\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGeneratedData(font=None, Fake=True):\n",
    "    f = open('/home/kuadmin01/terng/SeniorProjectMaterial/words_raw.txt')\n",
    "    datapath = []\n",
    "    label = []\n",
    "    fake_datapath = []\n",
    "    fake_datalabel = []\n",
    "\n",
    "    for line in f:\n",
    "        # ignore comment line\n",
    "        if not line or line[0] == '#':\n",
    "            continue\n",
    "\n",
    "        lineSplit = line.strip().split(' ')\n",
    "        fileNameSplit = lineSplit[0].split('-')\n",
    "        fileName =  '/home/kuadmin01/terng/HTR_dataset_word/' + \\\n",
    "        fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + \\\n",
    "        fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "\n",
    "        gtText = truncateLabel(' '.join(lineSplit[8:]), 128)\n",
    "        \n",
    "        # if len(gtText)>2:\n",
    "        if len(fileName)>0 and len(gtText)>0:\n",
    "            datapath.append(fileName)\n",
    "            label.append(gtText)\n",
    "            \n",
    "            if Fake:\n",
    "                generated_path = '/home/kuadmin01/terng/HTR_generated_dataset/' +font.split('.')[0]+'_'+ lineSplit[0] + '.png'\n",
    "                fake_datapath.append(generated_path)\n",
    "                fake_datalabel.append(gtText)\n",
    "            \n",
    "    if Fake:\n",
    "        return fake_datapath, fake_datalabel\n",
    "    else:\n",
    "        return datapath, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --user=terng03412 --password=1q2w3e4rterng http://www.fki.inf.unibe.ch/DBs/iamDB/data/words/words.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "DeepSpell based text cleaning process.\n",
    "    Tal Weiss.\n",
    "    Deep Spelling.\n",
    "    Medium: https://machinelearnings.co/deep-spelling-9ffef96a24f6#.2c9pu8nlm\n",
    "    Github: https://github.com/MajorTal/DeepSpell\n",
    "\"\"\"\n",
    "\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
    "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
    "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
    "\n",
    "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
    "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
    "\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
    "\n",
    "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
    "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
    "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
    "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
    "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
    "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
    "\n",
    "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
    "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
    "    text = text.translate(str.maketrans({c: f\" {c} \" for c in string.punctuation}))\n",
    "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sauvola binarization based in,\n",
    "    J. Sauvola, T. Seppanen, S. Haapakoski, M. Pietikainen,\n",
    "    Adaptive Document Binarization, in IEEE Computer Society Washington, 1997.\n",
    "\"\"\"\n",
    "\n",
    "def sauvola(img, window, thresh, k):\n",
    "    \"\"\"Sauvola binarization\"\"\"\n",
    "\n",
    "    rows, cols = img.shape\n",
    "    pad = int(np.floor(window[0] / 2))\n",
    "    sum2, sqsum = cv2.integral2(\n",
    "        cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_CONSTANT))\n",
    "\n",
    "    isum = sum2[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
    "        sum2[0:rows, 0:cols] - \\\n",
    "        sum2[window[0]:rows + window[0], 0:cols] - \\\n",
    "        sum2[0:rows, window[1]:cols + window[1]]\n",
    "\n",
    "    isqsum = sqsum[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
    "        sqsum[0:rows, 0:cols] - \\\n",
    "        sqsum[window[0]:rows + window[0], 0:cols] - \\\n",
    "        sqsum[0:rows, window[1]:cols + window[1]]\n",
    "\n",
    "    ksize = window[0] * window[1]\n",
    "    mean = isum / ksize\n",
    "    std = (((isqsum / ksize) - (mean**2) / ksize) / ksize) ** 0.5\n",
    "    threshold = (mean * (1 + k * (std / thresh - 1))) * (mean >= 100)\n",
    "\n",
    "    return np.asarray(255 * (img >= threshold), 'uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_cursive_style(img):\n",
    "    \"\"\"Remove cursive writing style from image with deslanting algorithm\"\"\"\n",
    "\n",
    "    def calc_y_alpha(vec):\n",
    "        indices = np.where(vec > 0)[0]\n",
    "        h_alpha = len(indices)\n",
    "\n",
    "        if h_alpha > 0:\n",
    "            delta_y_alpha = indices[h_alpha - 1] - indices[0] + 1\n",
    "\n",
    "            if h_alpha == delta_y_alpha:\n",
    "                return h_alpha * h_alpha\n",
    "        return 0\n",
    "\n",
    "    alpha_vals = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    rows, cols = img.shape\n",
    "    results = []\n",
    "\n",
    "    ret, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    binary = otsu if ret < 127 else sauvola(img, (int(img.shape[0] / 2), int(img.shape[0] / 2)), 127, 1e-2)\n",
    "\n",
    "    for alpha in alpha_vals:\n",
    "        shift_x = max(-alpha * rows, 0.)\n",
    "        size = (cols + int(np.ceil(abs(alpha * rows))), rows)\n",
    "        transform = np.asarray([[1, alpha, shift_x], [0, 1, 0]], dtype=np.float)\n",
    "\n",
    "        shear_img = cv2.warpAffine(binary, transform, size, cv2.INTER_NEAREST)\n",
    "        sum_alpha = 0\n",
    "        sum_alpha += np.apply_along_axis(calc_y_alpha, 0, shear_img)\n",
    "        results.append([np.sum(sum_alpha), size, transform])\n",
    "\n",
    "    result = sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "    warp = cv2.warpAffine(img, result[2], result[1], borderValue=255)\n",
    "\n",
    "    return cv2.resize(warp, dsize=(cols, rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img, input_size):\n",
    "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
    "    img_src = img\n",
    "    if isinstance(img, str):\n",
    "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if isinstance(img, tuple):\n",
    "        image, boundbox = img\n",
    "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        for i in range(len(boundbox)):\n",
    "            if isinstance(boundbox[i], float):\n",
    "                total = len(img) if i < 2 else len(img[0])\n",
    "                boundbox[i] = int(total * boundbox[i])\n",
    "\n",
    "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
    "\n",
    "    wt, ht, _ = input_size\n",
    "    try:\n",
    "        h, w = np.asarray(img).shape\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {img_src}\")\n",
    "        return\n",
    "\n",
    "    f = max((w / wt), (h / ht))\n",
    "\n",
    "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
    "    img = cv2.resize(img, new_size)\n",
    "\n",
    "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    if np.sum(img) * 0.8 > np.sum(binary):\n",
    "        img = illumination_compensation(img)\n",
    "\n",
    "    img = remove_cursive_style(img)\n",
    "\n",
    "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
    "    target[0:new_size[1], 0:new_size[0]] = img\n",
    "    img = cv2.transpose(target)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalization(imgs):\n",
    "    \"\"\"Normalize list of images\"\"\"\n",
    "\n",
    "    imgs = np.asarray(imgs).astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        m, s = cv2.meanStdDev(imgs[i])\n",
    "        imgs[i] = imgs[i] - m[0][0]\n",
    "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
    "\n",
    "    return np.expand_dims(imgs, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augmentation(imgs,\n",
    "                 rotation_range=0,\n",
    "                 scale_range=0,\n",
    "                 height_shift_range=0,\n",
    "                 width_shift_range=0,\n",
    "                 dilate_range=1,\n",
    "                 erode_range=1):\n",
    "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
    "\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
    "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
    "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
    "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
    "    scale = np.random.uniform(1 - scale_range, 1)\n",
    "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
    "\n",
    "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
    "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
    "\n",
    "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
    "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
    "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
    "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
    "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
    "\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def illumination_compensation(img):\n",
    "    \"\"\"Illumination compensation technique for text image\"\"\"\n",
    "\n",
    "    def scale(img):\n",
    "        s = np.max(img) - np.min(img)\n",
    "        res = img / s\n",
    "        res -= np.min(res)\n",
    "        res *= 255\n",
    "        return res\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    height, width = img.shape\n",
    "    sqrt_hw = np.sqrt(height * width)\n",
    "\n",
    "    bins = np.arange(0, 300, 10)\n",
    "    bins[26] = 255\n",
    "    hp = np.histogram(img, bins)\n",
    "    for i in range(len(hp[0])):\n",
    "        if hp[0][i] > sqrt_hw:\n",
    "            hr = i * 10\n",
    "            break\n",
    "\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    cei = (img - (hr + 50 * 0.3)) * 2\n",
    "    cei[cei > 255] = 255\n",
    "    cei[cei < 0] = 0\n",
    "\n",
    "    m1 = np.array([-1, 0, 1, -2, 0, 2, -1, 0, 1]).reshape((3, 3))\n",
    "    m2 = np.array([-2, -1, 0, -1, 0, 1, 0, 1, 2]).reshape((3, 3))\n",
    "    m3 = np.array([-1, -2, -1, 0, 0, 0, 1, 2, 1]).reshape((3, 3))\n",
    "    m4 = np.array([0, 1, 2, -1, 0, 1, -2, -1, 0]).reshape((3, 3))\n",
    "\n",
    "    eg1 = np.abs(cv2.filter2D(img, -1, m1))\n",
    "    eg2 = np.abs(cv2.filter2D(img, -1, m2))\n",
    "    eg3 = np.abs(cv2.filter2D(img, -1, m3))\n",
    "    eg4 = np.abs(cv2.filter2D(img, -1, m4))\n",
    "\n",
    "    eg_avg = scale((eg1 + eg2 + eg3 + eg4) / 4)\n",
    "\n",
    "    h, w = eg_avg.shape\n",
    "    eg_bin = np.zeros((h, w))\n",
    "    eg_bin[eg_avg >= 30] = 255\n",
    "\n",
    "    h, w = cei.shape\n",
    "    cei_bin = np.zeros((h, w))\n",
    "    cei_bin[cei >= 60] = 255\n",
    "\n",
    "    h, w = eg_bin.shape\n",
    "    tli = 255 * np.ones((h, w))\n",
    "    tli[eg_bin == 255] = 0\n",
    "    tli[cei_bin == 255] = 0\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(tli, kernel, iterations=1)\n",
    "    int_img = np.array(cei)\n",
    "\n",
    "    estimate_light_distribution(width, height, erosion, cei, int_img)\n",
    "\n",
    "    mean_filter = 1 / 121 * np.ones((11, 11), np.uint8)\n",
    "    ldi = cv2.filter2D(scale(int_img), -1, mean_filter)\n",
    "\n",
    "    result = np.divide(cei, ldi) * 260\n",
    "    result[erosion != 0] *= 1.5\n",
    "    result[result < 0] = 0\n",
    "    result[result > 255] = 255\n",
    "\n",
    "    return np.array(result, dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_light_distribution(width, height, erosion, cei, int_img):\n",
    "    \"\"\"Light distribution performed by numba (thanks @Sundrops)\"\"\"\n",
    "\n",
    "    for y in range(width):\n",
    "        for x in range(height):\n",
    "            if erosion[x][y] == 0:\n",
    "                i = x\n",
    "\n",
    "                while i < erosion.shape[0] and erosion[i][y] == 0:\n",
    "                    i += 1\n",
    "\n",
    "                end = i - 1\n",
    "                n = end - x + 1\n",
    "\n",
    "                if n <= 30:\n",
    "                    h, e = [], []\n",
    "\n",
    "                    for k in range(5):\n",
    "                        if x - k >= 0:\n",
    "                            h.append(cei[x - k][y])\n",
    "\n",
    "                        if end + k < cei.shape[0]:\n",
    "                            e.append(cei[end + k][y])\n",
    "\n",
    "                    mpv_h, mpv_e = max(h), max(e)\n",
    "\n",
    "                    for m in range(n):\n",
    "                        int_img[x + m][y] = mpv_h + (m + 1) * ((mpv_e - mpv_h) / n)\n",
    "\n",
    "                x = end\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = []\n",
    "label = []\n",
    "fake_datapath = []\n",
    "fake_datalabel = []\n",
    "font_list = ['alphabetizedCassetteTapes.ttf','ASensibleArmadillo.ttf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kuadmin01/terng/SeniorProjectMaterial/words_raw.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1e6a262620ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfont_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgenPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetGeneratedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfake_datapath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfake_datalabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ebc3f734fece>\u001b[0m in \u001b[0;36mgetGeneratedData\u001b[0;34m(font, Fake)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetGeneratedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/kuadmin01/terng/SeniorProjectMaterial/words_raw.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfake_datapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/kuadmin01/terng/SeniorProjectMaterial/words_raw.txt'"
     ]
    }
   ],
   "source": [
    "for f in font_list:\n",
    "    genPath, genLabel = getGeneratedData(f)\n",
    "    fake_datapath += genPath\n",
    "    fake_datalabel += genLabel\n",
    "    \n",
    "datapath, label = getGeneratedData(Fake=False)\n",
    "\n",
    "input_size = (1024, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kuadmin01/terng/HTR_dataset_word/a01/a01-000u/a01-000u-00-00.png'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fake_datalabel = fake_datalabel[:200]\n",
    "fake_datapath = fake_datapath[:200]\n",
    "datapath = datapath[:100]\n",
    "label = label[:100]\n",
    "\n",
    "path_to_font = '/home/kuadmin01/terng/Fonts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Generating dataset-------\n",
      "0\n",
      "data length --> 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('-------Generating dataset-------')\n",
    "\n",
    "for i in range(len(fake_datalabel)):\n",
    "    font_name = fake_datapath[i].split('/')[-1].split('_')[0] + '.ttf'\n",
    "    if(i%10000==0):\n",
    "        print(i)\n",
    "    generate(fake_datalabel[i],fake_datapath[i],  path_to_font+font_name)\n",
    "    \n",
    "# datapath += fake_datapath\n",
    "# label += fake_datalabel\n",
    "print('data length --> ' + str(len(datapath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list :  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "List after first shuffle  :  [29 55 95  7 13 92 16 74 21 20 76 68 60 52 27 78 86 24 14 96 42 49 85 40\n",
      " 94 77 75 93 69 72 37 73 23 66 65 45 28 97 80 33 64 81 11 62  9  6 82  2\n",
      " 70 54 35 89 48 22 12 34 53 50 63  4 25 98 90 51 87 43 88 32 83 31 56  3\n",
      " 41 67 91 19 61 18 17 58 59  0 79  5  1 26 84 39 99 44 46 15 10  8 36 38\n",
      " 30 71 47 57]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "number_list = np.arange(0, len(datapath))\n",
    "print (\"Original list : \",  number_list)\n",
    "\n",
    "random.shuffle(number_list) #shuffle method\n",
    "print (\"List after first shuffle  : \",  number_list)\n",
    "#train, validate, test = 0.7, 0.1, 0.2\n",
    "shuffled = number_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 20\n",
      "train : 70\n",
      "valid : 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_order = shuffled[:int(0.7*len(shuffled))]\n",
    "test_data_order = shuffled[int(0.7*len(shuffled)):int(0.9*len(shuffled))]\n",
    "valid_data_order = shuffled[int(0.9*len(shuffled)): int(len(shuffled))]\n",
    "\n",
    "print('test : ' + str(len(test_data_order)))\n",
    "print('train : '+ str(len(train_data_order)))\n",
    "print('valid : ' + str(len(valid_data_order)))\n",
    "\n",
    "partitions = ['train', 'test', 'valid']\n",
    "dataset = dict()\n",
    "data = [train_data_order, test_data_order, valid_data_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(partitions)):\n",
    "    dataset[partitions[i]] = {\"dt\": [], \"gt\": []}\n",
    "    for j in data[i]:\n",
    "        dataset[partitions[i]]['dt'].append(datapath[j])\n",
    "        dataset[partitions[i]]['gt'].append(label[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(partitions)): \n",
    "    gt = []\n",
    "    dt = []\n",
    "    for d, g in zip(dataset[partitions[i]]['dt'],dataset[partitions[i]]['gt']):\n",
    "        \n",
    "        txt = text_standardize(g).encode()\n",
    "        if(len(txt)>0):\n",
    "            gt.append(txt)\n",
    "            dt.append(preproc(d, input_size))\n",
    "            \n",
    "    \n",
    "    dataset[partitions[i]]['gt'] = gt\n",
    "    dataset[partitions[i]]['dt'] = dt\n",
    "    \n",
    "#     with h5py.File('/home/kuadmin01/terng/test_daaset.hdf5', \"a\") as hf:\n",
    "#         hf.create_dataset(f\"{i}/dt\", data=dataset[partitions[i]]['dt'], compression=\"gzip\", compression_opts=9)\n",
    "#         hf.create_dataset(f\"{i}/gt\", data=dataset[partitions[i]]['gt'], compression=\"gzip\", compression_opts=9)\n",
    "#         print(f\"[OK] {i} partition.\")\n",
    "\n",
    "# print(f\"Transformation finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "imgplot = plt.imshow(dataset['train']['dt'][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train partition.\n",
      "[OK] test partition.\n",
      "[OK] valid partition.\n",
      "Transformation finished.\n"
     ]
    }
   ],
   "source": [
    "for i in partitions:\n",
    "    with h5py.File('/home/kuadmin01/terng/test_daaset.hdf5', \"a\") as hf:\n",
    "        hf.create_dataset(f\"{i}/dt\", data=dataset[i]['dt'], compression=\"gzip\", compression_opts=9)\n",
    "        hf.create_dataset(f\"{i}/gt\", data=dataset[i]['gt'], compression=\"gzip\", compression_opts=9)\n",
    "        print(f\"[OK] {i} partition.\")\n",
    "\n",
    "print(f\"Transformation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd  terng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm test_daaset.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
